{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "e7h4PXQTL1MY"
      },
      "outputs": [],
      "source": [
        "# !pip3 install transformers\n",
        "!pip3 install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PJ9R6zl0QHH2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import openai\n",
        "openai.api_key = \"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\"\n",
        "\n",
        "def gpt3 (prompt):\n",
        "    response = openai.Completion.create(\n",
        "        model=\"text-davinci-003\",\n",
        "        prompt= prompt,\n",
        "        temperature=0.7,\n",
        "        max_tokens=256,\n",
        "        top_p=1,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "    return response['choices'][0]['text'].replace ('\\n', '').replace ('\\t', ' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-2Chsv0L5u7"
      },
      "outputs": [],
      "source": [
        "# functions for back translation (Reference - https://amitness.com/back-translation/)\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "\n",
        "target_model_name = 'Helsinki-NLP/opus-mt-en-fr'\n",
        "target_tokenizer = MarianTokenizer.from_pretrained(target_model_name)\n",
        "target_model = MarianMTModel.from_pretrained(target_model_name)\n",
        "\n",
        "en_model_name = 'Helsinki-NLP/opus-mt-fr-en'\n",
        "en_tokenizer = MarianTokenizer.from_pretrained(en_model_name)\n",
        "en_model = MarianMTModel.from_pretrained(en_model_name)\n",
        "\n",
        "\n",
        "def translate(texts, model, tokenizer, language=\"fr\"):\n",
        "    # Prepare the text data into appropriate format for the model\n",
        "    template = lambda text: f\"{text}\" if language == \"en\" else f\">>{language}<< {text}\"\n",
        "    src_texts = [template(text) for text in texts]\n",
        "\n",
        "    # Tokenize the texts\n",
        "    #encoded = tokenizer.prepare_seq2seq_batch(src_texts)\n",
        "    encoded = tokenizer.prepare_seq2seq_batch(src_texts,return_tensors=\"pt\")\n",
        "    \n",
        "    # Generate translation using model\n",
        "    translated = model.generate(**encoded)\n",
        "\n",
        "    # Convert the generated tokens indices back into text\n",
        "    translated_texts = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
        "    \n",
        "    return translated_texts\n",
        "\n",
        "\n",
        "def back_translate(texts, source_lang=\"en\", target_lang=\"fr\"):\n",
        "    # Translate from source to target language\n",
        "    fr_texts = translate(texts, target_model, target_tokenizer, \n",
        "                         language=target_lang)\n",
        "\n",
        "    # Translate from target language back to source language\n",
        "    back_translated_texts = translate(fr_texts, en_model, en_tokenizer, \n",
        "                                      language=source_lang)\n",
        "    \n",
        "    return back_translated_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1-IYJwQPV9k",
        "outputId": "7fd09c06-fd6f-4646-bd27-9b3e4658149e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/semantic_mapping_project\n"
          ]
        }
      ],
      "source": [
        "%cd drive/MyDrive/semantic_mapping_project/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bADlawGWbWhY"
      },
      "outputs": [],
      "source": [
        "def get_prompt(obj, house_id, curr_location, gpt3_room_description, direction, room_data):\n",
        "  prompt = \"I am owner of the house and I am aware of the location where important things are placed. I want to find {} in the house.\\n\\n\".format(obj)\n",
        "  prompt += \"House consists of {} bedrooms, {} bathroom, {} kitchen and {} Living Room.\\n\\n\".format(\n",
        "      len(room_data['Bedroom']), len(room_data['Bathroom']), \n",
        "      len(room_data['Kitchen']), len(room_data['LivingRoom']))\n",
        "  prompt += \"Currently I am in the {} now, and below is the description of current room.\\n\\n\".format(curr_location)\n",
        "  prompt += gpt3_room_description + \"\\n\\n\"\n",
        "  if direction in ['Left', 'Right']:\n",
        "    prompt += \"I remember that {} was present on the {} side from current position.\\n\\n\".format(obj, direction)\n",
        "  elif direction in ['BackwardRight', 'BackwardLeft']:\n",
        "    prompt += \"I remember that {} was present behind you, on the {} side from current position.\\n\\n\".format(obj, direction.split('Backward'[-1]))\n",
        "  prompt += \"Using the above information, Can you describe what should my intention in order to find {} ?\".format(obj)\n",
        "  return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_e6TEhhLbmff"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open('./rooms.json', 'r') as f:\n",
        "  room_data = json.load(f)\n",
        "with open('./house_context.json', 'r') as f:\n",
        "  house_context = json.load(f)\n",
        "with open('./mapping.json', 'r') as f:\n",
        "  mapping = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjI8tE8gkWi7"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "d = {}\n",
        "for key, house in mapping.items():\n",
        "  x = defaultdict(list)\n",
        "  for id, Rtype in house.items():\n",
        "    if type(Rtype) != str:\n",
        "      for i in Rtype:\n",
        "        x[i].append(id)\n",
        "    else: \n",
        "      x[Rtype].append(id)\n",
        "  d[key] = x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGKXYec5QTS9"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "for folder_name in tqdm(glob.glob(\"./\"+\"**_expert_annotations\",recursive=True)):\n",
        "  for file_name in tqdm(glob.glob(folder_name+\"/\"+\"forTraining/house_**_**.json\",recursive=True)):\n",
        "    with open(file_name, 'r') as f:\n",
        "      data = json.load(f)\n",
        "    for ac_id in tqdm(range(len(data['actions']))):\n",
        "      \n",
        "      house_id = 'house_' + str(data['house_id'])\n",
        "      curr_location = data['actions'][ac_id]['roomType']\n",
        "      room_options = d[str(data['house_id'])][curr_location]\n",
        "      room_idx = room_options[np.random.randint(low = 0, high = len(room_options))-1]\n",
        "\n",
        "      gpt3_room_description = house_context[house_id][0]['room_'+str(room_idx[-1])]['gpt3 summary']\n",
        "\n",
        "      direction = data['actions'][ac_id]['obj_direction']\n",
        "      prompt =  get_prompt(data['target_object_type'], data['house_id'], curr_location, gpt3_room_description, direction, d[str(data['house_id'])])\n",
        "\n",
        "      time.sleep(0.5)\n",
        "      response = gpt3(prompt)\n",
        "      time.sleep(0.5)\n",
        "      data['actions'][ac_id]['gpt3_intention_wd'] = response\n",
        "      \n",
        "    with open(file_name, \"w+\") as f:\n",
        "      json.dump(data, f)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}